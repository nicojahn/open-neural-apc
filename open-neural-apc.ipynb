{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow 2 implementation of open-neural-apc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2020-2021, Nico Jahn <br>\n",
    "All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extension to run open-neural-apc in google colab and retrieving all neccessary files\n",
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "if IN_COLAB:\n",
    "    import os\n",
    "\n",
    "    os.system(\"apt -qq update && apt -qq install git-lfs\")\n",
    "    os.system(\"git lfs install\")\n",
    "    os.system(\"git clone https://github.com/nicojahn/open-neural-apc.git\")\n",
    "    os.chdir(\"open-neural-apc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# install and update required packages\n",
    "python3 -m pip install --upgrade pip -q\n",
    "python3 -m pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_config, allow_growth\n",
    "\n",
    "allow_growth()\n",
    "# read the config file\n",
    "# it includes more or less all hyperparameter used in the model and preprocessing/training step\n",
    "data_parameter, model_parameter, training_parameter = load_config(verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing command line arguments and overwriting config if those are supplied\n",
    "from argument_parser import parse_arguments, overwrite_config\n",
    "\n",
    "parsed_arguments, _ = parse_arguments()\n",
    "overwrite_config(parsed_arguments, data_parameter, model_parameter, training_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Loading the training data \"\"\"\n",
    "from data_loader import DataLoader\n",
    "\n",
    "training_data = DataLoader(training_parameter, data_parameter[\"data\"], \"training\")\n",
    "\n",
    "if len(training_data) > 0:\n",
    "    \"\"\"Creating a model and save the config\"\"\"\n",
    "    from napc import NeuralAPC\n",
    "\n",
    "    # create model from config\n",
    "    napc = NeuralAPC(model_parameter, training_parameter)\n",
    "    napc.compile()\n",
    "    napc.save()\n",
    "\n",
    "    # writing config file into model folder\n",
    "    import json\n",
    "\n",
    "    new_config = {\n",
    "        \"data_parameter\": data_parameter,\n",
    "        \"model_parameter\": model_parameter,\n",
    "        \"training_parameter\": training_parameter,\n",
    "    }\n",
    "    with open(napc.config_path, \"w+\") as config:\n",
    "        config.write(json.dumps(new_config, sort_keys=True, indent=2))\n",
    "    print(f\"Model folder created and config saved: {napc.config_path}\")\n",
    "\n",
    "    \"\"\"Data generator initialization\"\"\"\n",
    "    from data_generator import DataGenerator\n",
    "\n",
    "    # this is class which preprocesses the training data every epoch\n",
    "    # it creates the necessary labels/bounds and augments the data\n",
    "    training_generator = DataGenerator(training_data, training_parameter, training=True)\n",
    "\n",
    "    \"\"\" Training procedure\"\"\"\n",
    "    napc.fit(\n",
    "        training_generator,\n",
    "        epochs=training_parameter[\"epochs\"],\n",
    "        initial_epoch=napc.epoch,\n",
    "        max_queue_size=len(training_generator),\n",
    "        workers=4,\n",
    "        use_multiprocessing=False,\n",
    "        callbacks=napc.callbacks,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" For all of you which don't have the training data and just want to execute the notebooks \"\"\"\n",
    "if len(training_data) == 0:\n",
    "    data_parameter, model_parameter, training_parameter = load_config(\n",
    "        \"models/config.json\", verbose=False\n",
    "    )\n",
    "    from napc import NeuralAPC\n",
    "\n",
    "    napc = NeuralAPC(model_parameter, training_parameter)\n",
    "    # Loading the included model (it has no subdirectory)\n",
    "    napc.load_model(epoch=10000, model_path=\"models/\")\n",
    "    # The model_path of the model is not 'models/', but the previously created subdirectory\n",
    "    # You could now train it further/save it/ etc.\n",
    "    napc.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Produce videos on all validation sequences or just validate the model \"\"\"\n",
    "# de-/activate video creation\n",
    "create_vids = False\n",
    "if create_vids:\n",
    "    ! apt -qq update && apt -qq install -y ffmpeg\n",
    "\n",
    "# copy dict from training and modify the concatenation\n",
    "validation_parameter = training_parameter.copy()\n",
    "validation_parameter[\"concatenation_length\"] = 1\n",
    "\n",
    "# read the validation data\n",
    "from data_loader import DataLoader\n",
    "\n",
    "validation_data = DataLoader(validation_parameter, data_parameter[\"data\"], \"validation\")\n",
    "\n",
    "# process them (i need the bounds in y for the accuracy and the videos)\n",
    "from data_generator import DataGenerator\n",
    "\n",
    "validation_generator = DataGenerator(\n",
    "    validation_data, validation_parameter, training=False\n",
    ")\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "accuracy = []\n",
    "for batch_idx in trange(len(validation_generator), desc=\"Batches done\", unit=\"batches\"):\n",
    "    # get batch, predict and calculate accuracy\n",
    "    x, y = validation_generator[batch_idx]\n",
    "    predictions = napc.predict_on_batch(x)\n",
    "    accuracy += [napc.accuracy(y, predictions)]\n",
    "\n",
    "    # creates my videos\n",
    "    if create_vids:\n",
    "        from video_generator import create_videos\n",
    "\n",
    "        create_videos(\n",
    "            x,\n",
    "            y,\n",
    "            predictions,\n",
    "            napc.epoch,\n",
    "            batch_idx * validation_parameter[\"batch_size\"],\n",
    "            data_parameter[\"class_names\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since I'm not allowed to upload more sequences and I don't want to publish a perfect model\n",
    "# (therefore, I haven't tested this one) the accuracy is just an approximation of the true capabilities\n",
    "# The 'validation' data in this case is in fact a test set (last epoch was chosen without selection)\n",
    "# In practice someone would use k-fold-CV and would reason about the average performance\n",
    "import numpy as np\n",
    "\n",
    "# So let's have a look how well the model does...\n",
    "print(f\"Accuracy: {100*np.mean(accuracy)} %\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
